/* AI í‚¤ê°€ í•„ìš”í•œë°.. ìš°ì„  ë„£ì–´ëŠ” ë†¨ìŠµë‹ˆë‹¤ */

/*
import type { NextApiRequest, NextApiResponse } from "next";
import OpenAI from "openai";

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

export default async function handler(req: NextApiRequest, res: NextApiResponse) {
  if (req.method !== "POST") {
    return res.status(405).json({ error: "POSTë§Œ í—ˆìš©ë©ë‹ˆë‹¤." });
  }

  const { messages } = req.body;

  if (!messages || !Array.isArray(messages)) {
    return res.status(400).json({ error: "messages ë°°ì—´ì´ í•„ìš”í•©ë‹ˆë‹¤." });
  }

  try {
    const completion = await openai.chat.completions.create({
      model: "gpt-3.5-turbo",
      messages,
    });

    const reply = completion.choices[0].message?.content || "ì‘ë‹µì´ ì—†ìŠµë‹ˆë‹¤.";
    res.status(200).json({ reply });
  } catch (err) {
    console.error(err);
    res.status(500).json({ reply: "OpenAI API í˜¸ì¶œ ì‹¤íŒ¨" });
  }
}

*/

// ì •í•´ì§„ ë‹µë³€
import type { NextApiRequest, NextApiResponse } from "next";

export default function handler(req: NextApiRequest, res: NextApiResponse) {
  if (req.method !== "POST") return res.status(405).json({ error: "POSTë§Œ í—ˆìš©ë©ë‹ˆë‹¤." });

  const { messages } = req.body;
  if (!Array.isArray(messages)) return res.status(400).json({ error: "messages ë°°ì—´ í•„ìš”" });

  // ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€ ê°€ì ¸ì˜¤ê¸° (ì›ë³¸ í›¼ì† X)
  const lastUserMsg = [...messages].reverse().find((m: any) => m.role === "user");
  const userText = lastUserMsg?.content?.toString() || "";

  let reply = "ì£„ì†¡í•´ìš”, ì´í•´í•˜ì§€ ëª»í–ˆì–´ìš”.";

  if (userText.includes("ì•ˆë…•")) reply = "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?";
  else if (userText.includes("ìƒë‹´")) reply = "í¸í•˜ê²Œ ë§ì”€í•´ ì£¼ì„¸ìš”. ì œê°€ í•¨ê»˜ ê³ ë¯¼ì„ ë“¤ì–´ë“œë¦´ê²Œìš”ğŸ™‚";
  else if (userText.includes("ê°ì‚¬")) reply = "ë³„ë§ì”€ì„ìš”. ë„ì›€ì´ ë˜ì—ˆë‹¤ë‹ˆ ê¸°ë»ìš”!";

  res.status(200).json({ reply });
}